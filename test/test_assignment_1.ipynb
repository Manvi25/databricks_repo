{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c97398-4f49-47ab-a17a-37edc6b9947d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: <function __main__.camel_to_snake(df)>"
     ]
    }
   ],
   "source": [
    "%run /Users/manvi.khandelwal@diggibyte.com/assignments/source_to_bronze/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1aa85c9-08f3-4272-8fb6-385f69ccef30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca09f673-7dfa-4435-b151-615d0d3ac0ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PTests(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.spark = SparkSession.builder.appName(\"PySpark Assignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8dfee02-cd0c-4bba-bc59-99d2b618f62f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " def test_read_csv(self):   \n",
    "        emp_path = 'dbfs:/FileStore/resources/Employee_Q1.csv'\n",
    "        dep_path = 'dbfs:/FileStore/resources/Department_Q1.csv'\n",
    "        country_path = 'dbfs:/FileStore/resources/Country_Q1.csv' \n",
    "        emp_df = read_csv(emp_path)  # Assuming read_csv function is defined correctly\n",
    "        dep_df = read_csv(dep_path)\n",
    "        country_path = read_csv(country_path)\n",
    "        self.assertTrue(emp_df.count() >0)  # Check if df contains 10 rows\n",
    "        self.assertTrue(dep_df.count() >0)\n",
    "        self.assertTrue(country_path.count() >0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be43a4b-84ec-4b1d-8cea-0ce0c7b64588",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " def test_custom_schema_read_csv(self):\n",
    "        test_input_schema = \"Emp_Id INT, Emp_Name STRING, department STRING, country STRING, salary INT, age INT\"\n",
    "        expected_output_schema = StructType([\n",
    "           StructField(\"Emp_Id\", IntegerType(), True),\n",
    "           StructField(\"Emp_Name\", StringType(), True),\n",
    "           StructField(\"department\", StringType(), True),\n",
    "           StructField(\"country\", StringType(), True),\n",
    "           StructField(\"salary\", IntegerType(), True),\n",
    "           StructField(\"age\", IntegerType(), True)\n",
    "         ])\n",
    "        result_df = custom_schema_read_csv(emp_path, test_input_schema)\n",
    "        # Assert that the schema of the resulting DataFrame matches the expected schema\n",
    "        self.assertEqual(result_df.schema, expected_output_schema)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be6e8eee-4b9b-43cc-9e3a-4de2b58e3812",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_camel_to_snake(self):\n",
    "        emp_df = custom_schema_read_csv('dbfs:/FileStore/resources/Employee_Q1.csv',employee_schema)\n",
    "        result_df = camel_to_snake(emp_df)\n",
    "\n",
    "        # Assert that column names are converted to snake case\n",
    "        expected_columns = [\"emp_id\", \"emp_name\", \"department\" , \"country\" , \"salary\" , \"age\"]\n",
    "        self.assertEqual(result_df.columns, expected_columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b217ea1-cbe6-48ee-aca5-bdafd150d350",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_current_date_df(self):\n",
    "        \n",
    "        emp_df = custom_schema_read_csv('dbfs:/FileStore/resources/Employee_Q1.csv',employee_schema)\n",
    "\n",
    "        # Apply the function on the  DataFrame\n",
    "        result_df = current_date_df(emp_df)\n",
    "\n",
    "        # Assert that the 'load_date' column is added and contains the current date\n",
    "        current_date_value = datetime.now().date()\n",
    "        self.assertTrue(\"load_date\" in result_df.columns)\n",
    "        self.assertEqual(result_df.select(\"load_date\").first()[0], current_date_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b0ab0e-5568-456e-b128-a97143b2fa0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " def test_salary_of_each_department(self):\n",
    "        emp_df = custom_schema_read_csv('dbfs:/FileStore/resources/Employee_Q1.csv',employee_schema)\n",
    "\n",
    "        # Apply the function on the sample DataFrame\n",
    "        result_df = salary_of_each_department(emp_df)\n",
    "\n",
    "        # Assert that the resulting DataFrame contains the expected columns\n",
    "        expected_columns = [\"department\", \"total_salary\"]\n",
    "        self.assertEqual(result_df.columns, expected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b33333d-ec2e-4a47-87d9-552d1cc241ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_employee_count(self):\n",
    "        # Create a sample DataFrame for testing\n",
    "        emp_df = custom_schema_read_csv('dbfs:/FileStore/resources/Employee_Q1.csv',employee_schema)\n",
    "        # Apply the function on the sample DataFrame\n",
    "        result_df = employee_count(emp_df)\n",
    "\n",
    "        # Assert that the resulting DataFrame contains the expected columns\n",
    "        expected_columns = [\"department\", \"country\", \"count\"]\n",
    "        self.assertEqual(result_df.columns, expected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c76c28b-f1f6-4c4e-91e2-2a8418da2129",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_list_of_department(self):\n",
    "        emp_df = custom_schema_read_csv('dbfs:/FileStore/resources/Employee_Q1.csv',employee_schema)\n",
    "\n",
    "        # Apply the function on the DataFrame\n",
    "        result_df = list_the_department(emp_df)\n",
    "\n",
    "        # Assert that the resulting DataFrame contains the expected columns\n",
    "        expected_columns = [\"department\", \"country\"]\n",
    "        self.assertEqual(result_df.columns, expected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fab05c9-81c7-4541-9fdf-df8033c8b412",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " def test_avg_age(self):\n",
    "        emp_df = custom_schema_read_csv('dbfs:/FileStore/resources/Employee_Q1.csv',employee_schema)\n",
    "\n",
    "        # Apply the function on the DataFrame\n",
    "        result_df = avg_age(emp_df)\n",
    "\n",
    "        # Assert that the resulting DataFrame contains the expected columns\n",
    "        expected_columns = [\"department\", \"average_age\"]\n",
    "        self.assertEqual(result_df.columns, expected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390b16a4-c682-43c3-bfc7-17e5186ffae0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n----------------------------------------------------------------------\nRan 0 tests in 0.000s\n\nOK\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: <unittest.runner.TextTestResult run=0 errors=0 failures=0>"
     ]
    }
   ],
   "source": [
    "suite = unittest.TestLoader().loadTestsFromTestCase(PTests)\n",
    "unittest.TextTestRunner(verbosity=1).run(suite)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_assignment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
